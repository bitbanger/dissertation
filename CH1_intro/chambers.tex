\subsection{Unsupervised Learning of Narrative Event Chains}
\label{sec:chambers}

One of the earlier influential papers on statistical script learning is \textit{Unsupervised Learning of Narrative Event Chains} \citep{chambers2008unsupervised}. This paper defined, and sought to learn, \textit{narrative event chains}: a set of events with a mutual ``protagonist'' and a partial temporal order. The events in the chains were modeled as 2-tuples of verbs and entities.

\subsubsection{Event Model}
The authors defined events as 2-tuples of of the form \textit{(event, dependency)}, where \textit{event} is a verb, and \textit{dependency} is an entity---the protagonist---as either the \textit{subject} or \textit{object} of the verb. This latter element of the tuple is called a \textit{typed dependency}.

\subsubsection{The Learning Method}
The authors use a three-stage approach to learning event chains:

\begin{enumerate}
\item First, they perform \textbf{narrative event induction} on the syntactically parsed text corpus: events are extracted from the parsed clauses. Then, they calculate, using approximate pointwise mutual information (PMI), a pairwise relationship metric
%\footnote{More specifically: the pointwise mutual information of the outcomes of two random variables---here, event representations drawn from a distribution of observed ones---is a quantification of the effect of statistical dependence in the co-incidence of both outcomes.}
for the set of events, based on the co-occurrence of shared grammatical arguments---the hypothesis here is that shared arguments imply co-occurrence of the events in a narrative chain. The pairwise event co-occurrence scores can then be combined to find the next most likely event in a chain given all the events in the chain so far by maximizing $\max\limits_{{j:0<j<m}} \sum\limits_{i=0}^{n} pmi(e_{i}, f_{j})$\footnote{N.B.: maximizing this quantity is equivalent to picking the event $f_{j}$ from the training corpus with the most occurrences of shared co-referring entity arguments with $e_{i}$, as can be inferred from the definitions to follow.}, where $e_{i}$ is the $i$-th event in a chain of $n$ events, candidate event $f_{j}$ is the $j$-th event in the training corpus of $m$ events, and $pmi(e_{i}, f_{j})$ is an approximation of the pointwise mutual information of $e_{i}$ and $f_{j}$.\footnote{N.B.: they only need to calculate pointwise mutual information here, rather than the full distributional mutual information, because of their ``Markov assumption'' that an event's likelihood is predicted only by the last event.}

The approximate pointwise mutual information of the two events is defined by $$pmi(e_{i}, f_{j}) = \log \frac{P(e_{i}, f_{j})}{P(e_{i}) P(f_{j})}$$

The marginal event probabilities in the denominator of the PMI approximation are given simply by the observed frequencies of those events in the training corpus. The approximated pairwise joint event distribution in the numerator is defined by $$P(e_{i}, f_{j}) = \frac{C(e_{i}, f_{j})}{\sum_{e1}\sum_{e2} C(e1, e2)}$$ where, for any pair of events $e1$ and $e2$ having respective words $w1$ and $w2$ and respective dependencies $d1$ and $d2$, $C(e1, e2)$ is the number of times the dependencies $d1$ and $d2$ were filled by co-referring entities. Put more simply: this PMI approximation assumes that any statistical dependence in the joint distribution is due to shared argument entities, and thus quantifies the effect of entity co-reference in event co-occurrence. Put \textit{even more} simply: an event is more likely to come next if it shares arguments with the last event.
%\footnote{Put \textit{comedically} simply: ``yeah, that makes sense, I recognize that guy from before''.}

\item After the narrative event chain distribution is induced, they compute \textbf{partial temporal ordering} by using support vector machines (SVMs) to predict the presence of a \textit{before} relation of two events using syntactic information, including verb tense and grammatical aspect. The training data for the SVM was taken from the Timebank corpus of event relations \citep{pustejovsky2003timebank}, whose relations were simplified down into the \textit{before} relation used by this paper, and an \textit{other} relation.

\item Finally, they use an agglomerative clustering algorithm---in which events begin with their own clusters, which are then merged based on a similarity metric---to \textbf{identify the event chains}. The clustering algorithm uses the pointwise mutual information from the first step as a similarity score. Once the chains are constructed, their temporal ordering is induced by the SVM model in the second step.
\end{enumerate}

\subsubsection{Experimental Setup}
\label{sec:narrativecloze}
The authors used the Gigaword corpus \citep{parker2011english} as a document source for their experiments. The protagonist for each document is the entity with the highest number of mentions---they used the OpenNLP co-reference engine \citep{baldridge2005opennlp} for entity recognition.  Evaluation was performed using a metric the authors introduced in this paper: the \textbf{narrative cloze} task, an extension to narratives of the cloze test defined by \citep{taylor1953cloze}, in which humans were tasked with filling in missing words in texts. In the narrative cloze test, the system fills missing event tuples into narrative chains.

\subsubsection{Results}
They defined the baseline for their evaluation as a version of their model that did not use the protagonist, or its relation to each event as a subject or object, in making predictions. As the baseline could then only produce verb event chains, rather than 2-tuple event chains, they then modified their experimental model to use the protagonist co-reference information during training, but to omit it in the output chains. It is unclear to me, from reading the paper,  where the ``golden'' event data for each document they used to compute accuracy came from. They reported a nearly \textbf{37\%} accuracy improvement relative to the baseline, demonstrating the efficacy of the typed protagonist information. Removing the OpenNLP co-reference resolution step caused accuracy to drop by \textbf{5.7\%}.

\subsubsection{Discussion}
This was a hugely influential paper in statistical script learning, and while the authors themselves note that the event sequences are a far cry from the richness of Schank \& Abelson's scripts, as defined in \citep{schankandabelson}, they did pave the way for more expressive event models in future approaches, such the 5-tuples in \citep{pichotta2016learning}, which I describe in Section~\ref{sec:pichotta}.



% See \citep{pichotta2016learning}.