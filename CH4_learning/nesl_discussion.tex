\section{Discussion}
\label{sec:nesl_discussion}

NESL is a first step toward EL schema learning, leveraging neural protoschema matching, LSS with large language models, and symbolic generalization algorithms to produce generalized schemas for situation types on demand. However, NESL is not a complete solution to the problem of few-shot schema learning: the language model used to power LSS has seen many ``shots'', and topic-parameterized story generation is not scalable with human authors. Collections of stories found ``in the wild'', too, may not share the same underlying ``main schemas'', and it may be hard to identify and cluster those stories that do. For these reasons, LSS is not immediately amenable to the removal and replacement of large language models in its architecture. The goal of schema learning from a small number of real, human-generated stories, encountered naturally, therefore remains unattained. What further work, then, could facilitate its attainment? While definite answers cannot be given here, I will discuss a few of the issues, and potential resolutions, here.

%One issue I highlighted above is that assembling collections of stories with the same underlying topics is difficult without \textit{a priori} knowledge, as we have when generating stories from a language model, of the topics they instantiate.
% Talk about work on topic clustering
% Talk about work on topic summarization

One issue is the identification and selection of only salient events in a story in the absence of many other examples of stories instantiating the same schema. Heuristics are necessary here. One such heuristic is based on \textit{causal chaining}: after identifying protoschemas in a story, any pairs of actions in the story whose preconditions and postconditions unify with one another in the correct temporal order can be inferred to be causally connected. Unifying a postcondition with a goal is another form of causal chaining. Transitive closures of these connections, or ``chains'', can be inferred to be important to the story and likely involved in its main underlying schemas. Another means of few-shot schema identification is to increase the number of initial protoschemas. Some seemingly unconnected formulas in a story may actually be constraints, preconditions, postconditions, etc. of protoschemas, whose unification with the text would serve as an indicator of salience of all formulas matched to them. In general, though---especially in the early stages of schema learning, when few specific schemas are known---the problem of salience filtering from a single example is often intractable, and waiting until similar stories are encountered may be the only means of doing so.

Finally, NESL-learned schemas do not have the full expressive power EL schemas are meant to support. The current model of learned schemas does not incorporate 2D and 3D imagistic protoschemas for identifying and reasoning about basic kinds of objects \textit{and their uses}; the representation of process flows, e.g. repetitions, iterations, and branching, in the schema steps; or simulative or planning capabilities for applying schemas to, or learning schemas from, external or simulated environments. EL schemas are designed to bring the full richness of symbolic reasoning in all of these domains to bear in the fight for universal knowledge representation, and work is still required on all of those fronts.