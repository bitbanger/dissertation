\section{Schema Learning}
\label{sec:protoschemas}
What sets our approach to schema learning apart from past approaches is its focus on learning schemas more like a human child does. Human children learn simple schemas like ``sharing toys'' at young ages, but rapidly extend the underlying themes to describe new stereotyped concepts like ``time-shared condominiums'' as they age, by both generalizing out specific fillers, like ``toys'', and specifying concepts, like ``sharing'' to ``time-sharing''. But each schema they learn is a modification to a schema whose overall structure already makes sense to them: the underlying theme of ``sharing'' is really not so different, and might even be inferred from the similarity of the word, or by noticing the similar situation of two people using the same thing. Many of these schemas derive from an initial set of behaviors and tendencies. These initial schemas do not all need to be biologically ``built in'': concepts like sharing are likely learned, but are nevertheless learned quite young. We suggest that the initial set of protoschemas be roughly equivalent to what the average two-year-old child would know, with some examples being:

\begin{enumerate}
    \item Doing something to enable doing something else
    \item Gaining new information
    \item Asking for someone's help to enable doing something
    \item Some basic Schankian ``primitives'': basic knowledge of eating, grasping, sleeping, etc.
\end{enumerate}

%EL schema learning from text is bootstrapped with a set of simple, general schemas---\textit{\textbf{protoschemas}}---that are meant to provide wide coverage of nearly all events and actions represented in texts.
This conceptual ``head start'' mirrors prior knowledge observable even in the youngest humans: young toddlers begin learning about the world in terms of basic actions they already understand quite well, such as the manipulation of objects, the communication of information and requests, and the \textit{use} of those actions in order to attain innate \textit{goals}, like the alleviation of hunger or taking possession of a coveted object. As people age, they acquire schemas detailing progressively more complicated workings of the world, but do so on the basis and in terms of already-known schemas: even situations as complex as global economic supply chain disruptions can generally be understood in terms of basic situation types, e.g. the desire and transportation of objects and the limits of containers.
% Something about how we "cut things off" at 18-24 months to dodge questions of innate-ness
% Cite Piaget
A suitably general set of schemas can, in principle, cover all or most events in English text at some level of generality; even basic generalization of event and action verbs into synonym classes vastly reduces the number of possible linguistic expressions of events. We can further reduce this space with the introduction of a specialization hierarchy and the ability, given that hierarchy, to generalize more specific verbs, e.g. \textit{escape}, into more general verb \textit{classes}, e.g. \textit{movement}.
%Such generalization, of course, comes with information loss, and so, under the fixed constraint that most or all text be understandable, a trade-off is introduced between completeness of initial understanding and manual effort required to craft that initial understanding.

Verbs and verb hierarchies alone do not fully describe the space of even simple events; even when events can be characterized by one verb, the verb's modifiers and arguments must be considered, as well as the types and inter-relations of the arguments. Additionally, information about preconditions, effects, and inherent goals are associated with even basic event types, and can be used to detect implied events in text, e.g. \textit{Jack wanted to win} $\Rightarrow$ \textit{Jack was competing}.
This is why we seek to establish a set of proto-\textit{schemas} to provide head start knowledge.
Single verbs can \textit{represent} most actions and events, though, and thus act as the predicate of EL schema header propositions. With this in mind, we move on to practical investigation of the core question of protoschemas: \textit{how many, and what types, of protoschemas are necessary to bootstrap schema learning from simple texts}?

\subsection{Investigating Protoschema Coverage}
In constructing a set of protoschemas to abstractly cover most or all text during learning, a trade-off must be made between generality and required schema construction effort. On one end of the trade-off, a single, highly general ``something happens'' schema covers all events. On the other end, all verbs require their own manually-constructed schema. To help select a balance point of this trade-off, we investigate the relationship between two corpora: one of existing, hand-written frames, and one of simple child-level stories. We use the FrameNet frame corpus \citep{framenet}, which let us establish two bounds: one upper bound on what amount of manual construction effort is feasible, and one lower bound on the quality and ``interestingness'' of schemas that can be matched to text.
% TODO: describe rocstory corpus here
If many events in the story corpus are unmatchable to FrameNet frames, then the construction of a protoschema corpus may be prohibitively expensive. If most of the events are matchable, however, then it is possible to achieve at least the semantic specificity of FrameNet frames while expending comparable effort.

\subsubsection{Experimental Setup}
Our story set is derived from the ROCstories corpus \citep{mostafazadeh-etal-2016-corpus}. To filter based on conceptual simplicity, we selected a subset of ROCstories for this investigation by sorting all stories by the proportion of their non-stopwords that co-occurred in a set of children's first reader stories \citep{mcguffey}, and then randomly selecting 300 of the 500 stories with the highest such proportions.\footnote{The other 200 were held out for use in \textit{Latent Schema Sampling}, which is discussed in Chapter~\ref{chap:learning}.} To identify the FrameNet frames in this story set, we opted to use a state-of-the-art, neural network-based FrameNet parser, LOME \citep{lome}, as manual annotation on this number of stories was not practical.

As our coverage metric, we evaluated the number of root verbs in the story corpus that invoked FrameNet frames via LOME. To identify the verbs in the corpus, we examined each token's part-of-speech tag using spaCy \citep{spacy2}. After obtaining all frame matches for the corpus, we sorted the matched frame names by the number of verbs they covered, and then re-calculated the coverage metric, for all $K$, using only the top-$K$ most-covering frame names---this gives the marginal value of including each frame. To control for the well-known tendency of a small number of unique verbs to cover a large proportion of verb instances in naturally occurring text \citep{zipf-nl}, we only allowed each frame name to count one match for each unique verb name.

\subsubsection{Results}
\begin{figure}
    \centering
    \includegraphics[width=0.75\columnwidth]{CH3_schemas/framenet_coverage}
    \caption{The results of an investigation into the number of verbs in a corpus that can be matched to FrameNet frames. A total of 91\% of verbs can be matched using automated FrameNet matching techniques. The region shaded in red shows that 80\% of all matched verbs can be covered by only 57\% of the total frames invoked across the corpus. The best-fit logarithm is overlaid in green for reference.}
    \label{fig:fn_coverage}
\end{figure}

Figure~\ref{fig:fn_coverage} shows the results of this experiment. In total, 91\% of all unique verbs in the story corpus were covered by frames identified by LOME. The number of unique frames identified by LOME was 175---far fewer than FrameNet's total number of frames, which exceeds 1,200. Furthermore, even when controlling for the frequency of each verb, a logarithmically increasing relationship between the number of frames used and the number of verbs matched is still apparent, providing further evidence that only a small number of initial frames, or schemas, are necessary to cover most text at FrameNet's average level of generality. The suitability of FrameNet's frames to this task inspired later work on using the LOME FrameNet parser for protoschema identification (see Section~\ref{sec:lome}), as many of our conceptual protoschemas correspond to FrameNet frames.