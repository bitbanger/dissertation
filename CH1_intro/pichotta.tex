\section{Learning statistical scripts with LSTM recurrent neural networks}
\label{sec:pichotta}

Karl Pichotta and Raymond Mooney's paper \textit{Learning statistical scripts with LSTM recurrent neural networks} \citep{pichotta2016learning} was the first application of long short-term memory (LSTM) neural networks to the task of script learning. LSTM units, introduced by \citep{lstms}, can be thought of as memory cells that learn when to store, and when to forget, input items. Recurrent neural networks---that is, neural networks whose neuron connection graphs contain a cycle--enhanced with LSTM units are extremely powerful sequence learners, and have been applied to such diverse tasks as handwriting recognition \citep{graves2009offline}, machine translation \citep{sutskever2014sequence}, and music generation \citep{eck2002finding}. Hoping that the memory cells would help learn long-range narrative structure in event sequences, Pichotta and Mooney applied them here to script learning.

\subsection{Event Model}

Pichotta and Mooney extend the 2-tuple event representation from \citep{chambers2008unsupervised} into a 5-tuple representation of the form $(v, e_{s}, e_{o}, e_{p}, p)$, where $v$ is a lemmatized verb (i.e. the naked dictionary-form verb); three nouns $e_{s}$, $e_{o}$, and $e_{p}$ are related to the event as the subject, object, and prepositional attachment of $v$, respectively; and $p$ is the preposition that attaches $e_{p}$ to $v$. Each element of the tuple is known as an \textit{event component}, and, excepting $v$, may be \textit{null} (i.e. unspecified). (Note that $e_{p}$ is unspecified if and only if $p$ is unspecified; they are coupled event components.)

The authors define two types of script model from these event tuples. \textbf{Noun models} predict the verb, noun, and preposition lemmas of events in a sequence, as a generative model. \textbf{Entity models} predict almost the same sequences, but substitute \textit{entity IDs} for nouns. Entity IDs are unique integers representing entities with multiple mentions in the text, and are derived from a co-reference analysis. They note that prior script learning systems, e.g. \citep{chambers2008unsupervised}, had all been entity models.

Given these script model definitions, they define four separate architectures, each trained independently:

\begin{enumerate}
    \item \textbf{LSTM-noun-noun}: predicts argument nouns given argument nouns.
    \item \textbf{LSTM-ent-ent}: predicts entity IDs given entity IDs.
    \item \textbf{LSTM-both-noun}: predicts argument nouns given both argument nouns and entity IDs.
    \item \textbf{LSTM-both-ent}: predicts entity IDs given both argument nouns and entity IDs.
\end{enumerate}

\subsection{The Learning Method}
Because LSTMs make their sequential predictions ``one step at a time'', each tuple must take five timesteps to predict. Each timestep's input comprises several one-hot vectors (i.e. vectors of all 0s, except for one 1), which are assigned continuous distributional representations, a.k.a. \textit{embeddings} (similarly to the one-hot vocabulary vectors in \citep{mikolov2013}). The first one-hot vector gives the current tuple index, ranging from 0 to 5; this modular sequence is easy for the LSTM to learn, and ensures that the timesteps output valid groups of 5 tuple elements. The second one-hot vector represents the vocabulary word at that index. Finally, the third one-hot vector---present in all architectures except for \textbf{LSTM-noun-noun}---represents the entity ID at the current timestep.

The embeddings of these vectors are given as input to the LSTM network, which produces an output vector of the same length as the vocabulary. The vector is normalized into a probability distribution by a softmax function. The network is trained by back-propagating an error value calculated with the \textit{cross-entropy method} \citep{rubinstein2013cross}, in which the cross-entropy---roughly, the average number of bits needed to encode an outcome from some distribution $Q$, weighted by the probability of that outcome in another distribution $P$---between the output distribution and the target distribution is iteratively minimized for each training sample. Entity models also produce a probability distribution over the next entity ID in the same way. Likely tuples must be constructed from the most likely sequences of samples from five consecutive word and entity distributions; finding the most likely such sequence is intractable, as there are $|V|$ entries in each probability distribution (where $V$ is the vocabulary set), for a total search time bounded by $O(5^{|V|})$. So, for a more tractable approximation, likely sequences of 5 observations are generated using a five-step beam search.

\subsection{Narrative Cloze Experiment}
The first evaluation was performed on the narrative cloze model introduced by \citep{chambers2008unsupervised} (and described here in Section~\ref{sec:narrativecloze}). Because they defined a new event representation, the authors had to define their baseline against which to evaluate their architectures. They actually defined four baselines:

\begin{enumerate}
    \item The \textbf{unigram model} ignores the document and samples events by their frequency in the training set. They note that \citep{pichmoon2014} showed this baseline to be ``surprisingly competitive''. They created a noun unigram model and an entity unigram model to cover all of their architectures.
    
    \item The \textbf{all-bigram model} constructs statistics of event ``skip'' bigrams, where up to two events were allowed to ``intervene'' in between the events in the bigram. This is called a 2-skip bigram model, and is an instance of the more general skip n-gram model developed for script learning systems by \citep{jans2012skip}. The 2-skip bigram model models the probability of an event in the sequence given the previous event in the sequence, and can be thought of as a Markov approximation of the full joint distribution. To generate an inference for some position $t$ in the sequence, the authors maximize the objective function $$S(a) = \sum\limits_{i=0}^{t-1} \log P(a|s_{i})$$ where $s_{i}$ represents the $i$-th event of the sequence, and $P(b|a)$ is the probability of event $b$ given the previous event $a$, given by the 2-skip bigram model.
    
    \item The \textbf{rewritten all-bigram model} is a version of the all-bigram model with different behavior for co-occurring events, that is, events with co-occurring entities. For all co-occurring events $a$ and $b$ in the training process, the training data is augmented with all possible event pairs equivalent to $a$ and $b$ except for some subset of the shared entity IDs being re-written as other entity IDs. The intuition here, the authors say, is that co-occurrence relationships between events are likely to be \textit{generalizable} to other entities. Inferences are generated by maximizing $S(a)$, as they were in the all-bigram model.
    
    \item The \textbf{2D rewritten all-bigram model} is like the rewritten all-bigram model, except it generates inferences by maximizing the objective function $$S_{2}(a) = \sum\limits_{i=0}^{t-1} \log P(a|s_{i}) + \sum\limits_{j=t+1}^{l} \log P(s_{j}|a)$$ where $l$ is the sequence length. (Note that the first term of $S_{2}(a)$ is the definition of $S(a)$ from the all-bigram model.) The idea here is to incorporate not just the probability of an event following some event $s_{i}$, as in $S(a)$, but also the probability of that event \textit{preceding} some event $s_{j}$.
\end{enumerate}

The authors found \textbf{LSTM-both-noun} to be the best noun model, and \textbf{LSTM-both-end} to be the best entity model, indicating that receiving noun and entity information together gives the strongest performance. The full table of their results is not reproduced here, as it is not particularly useful for our discussion; it can be found in \citep{pichotta2016learning}.

\subsection{Experimental Setup for Mechanical Turk}
Citing the inherent difficulty of the narrative cloze task, the authors also evaluated the likelihoods of their event inferences using annotators on Amazon Mechanical Turk. They presented the annotators with English story snippets and five examples of event inferences, four of which were generated by the model being evaluated, and one of which was always generated by a randomly chosen event from the set of 10,000 most frequent events. The annotators rated the likelihoods of each of the five inferences on a scale from 0 (``Nonsense'') to 5 (``Very Likely''). They collected three annotator ratings for each event inference. Inferences were generated for each of four systems: \textbf{LSTM-both-noun}, \textbf{All-bigram-noun}, \textbf{LSTM-both-ent}, and \textbf{All-bigram-ent}. They calculated a ``filtered'' overall score for each model by discarding ratings from annotators whose average score for the randomly chosen event was greater than 1 (``Very Unlikely/Irrelevant'').

The authors found that both LSTM models outperformed both bigram models, with filtered average ratings of 3.67 for \textbf{LSTM-both-noun} and 3.08 for \textbf{LSTM-both-ent}. The average filtered scores for the baseline models were 2.21 for \textbf{All-bigram-noun} and 2.87 for \textbf{All-bigram-ent}. The filtered score for the random events was 0.87.

\subsection{Discussion}
Pichotta and Mooney made two notable contributions here: (1) they presented a richer model than \citep{chambers2008unsupervised} by both extending the event representation and applying script learning techniques suggested by \citep{jans2012skip}; and, (2) they demonstrated the effectiveness of LSTMs in learning script sequences. I've represented some examples of learned sequences in Figure~\ref{fig:pichottascript}.

Though the script sequences in that figure appear to tell stories, it's important to note that important semantic information has been introduced by the authors into the grammar of the interpretations. For example, the event tuple bigram \texttt{(capture, squad, villager, ., .)} \texttt{(give, inhabitant, group, ., .)} was interpreted as a continuous sentence, but ``give'' is a multi-argument verb: an equally valid interpretation of the latter event tuple could be ``The inhabitants gave a group'', with ``group'' filling in for the object-given argument rather than the recipient argument. Of course, one could imagine solving such ambiguities using semantic role disambiguation tools, making use of semantic corpora such as FrameNet (Section~\ref{sec:framenet}), but the fundamental issue of the restrictive event representation would remain.

However, despite the limited event representation, the authors have shown that LSTMs can learn interesting event structures from large text corpora, and one can imagine several uses and extensions. Perhaps using a conceptual hierarchy to generalize entities at the word-level could help create more general scripts from lone story examples? Or maybe the generated event sequences could be used as suggestions for an explanation-based learning system with a richer event representation to elaborate on. This system invites many interesting extensions.

\begin{figure}
\makebox[\textwidth][c]{\begin{footnotesize}
  \begin{tabular}{|l l|}
    \hline
    \textbf{Generated event tuples} & \textbf{Authors' interpretations} \\ \Xhline{4\arrayrulewidth}
    \texttt{(pass, route, creek, north, in)} & The route passes the creek in the North \\
    \texttt{(traverse, it, river, south, to)} & It traverses the river to the South \\ \hline
    \texttt{(issue, ., recommendation, government, from)} & A recommendation was issued from the government \\
    \texttt{(guarantee, ., regulation, ., .)} & Regulations were guaranteed \\
    \texttt{(administer, agency, program, ., .)} & The Agency administered the program \\
    \texttt{(post, ., correction, website, through)} & A correction was posted through a website \\
    \texttt{(ensure, standard, ., ., .)} & Standards were ensured \\
    \texttt{(assess, ., transparency, ., .)} & Transparency was assessed \\ \hline
    \texttt{((establish, ., ., citizen, by)} &  Established by citizens, ... \\
    \texttt{(end, ., liberation, ., .)} &  ... the liberation was ended \\
    \texttt{(kill, ., man, ., .)} &  A man was killed \\
    \texttt{(rebuild, ., camp, initiative, on)} &  The camp was rebuilt on an initiative \\
    \texttt{(capture, squad, villager, ., .)} &  A squad captured a villager ... \\ 
    \texttt{(give, inhabitant, group, ., .)} &  ... [which] the inhabitants had given the group \\ \hline
  \end{tabular}
  \end{footnotesize}}
\caption{Examples of probabilistically generated scripts from the LSTM model given by \citep{pichotta2016learning}, also showing those authors' English interpretations of the event tuples.}
\label{fig:pichottascript}
\end{figure}