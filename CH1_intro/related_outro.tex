\section{Discussion}
\label{sec:related_disc}
We have now seen examples of classical schema modeling in Schank and Abelson's scripts (Section~\ref{sec:schank}) and FrameNet (Section~\ref{sec:framenet}), as well as classical schema learning in IPP (Section~\ref{sec:ipp}). These systems, in their representational richness, lack scalable and modernized learning algorithms, and broadly mark the end of the era of purely symbolic schema learning. We have also seen examples of modern, statistical schema learning, which began with Chambers and Jurafsky's event scripts (Section~\ref{sec:chambers}) and continued into the neural network epoch with work by Pichotta and Mooney (Section~\ref{sec:pichotta}). As work is ongoing in the ``statistical camp'', I will briefly mention here some even more modern work: \citet{weber_causal_scripts} introduce causally-inspired nuance into the metric by which script events are correlated, but continue to employ the simple, tuple-based event representation of Chambers and Jurafsky. \citet{proscript} employ pre-trained language models to generate graphical script-like knowledge, but the events are represented as natural language text fragments, limiting the structural richness and direct inferential capacity of the knowledge. \citet{starsem-scripts} also present a system for learning scripts from pre-trained language models, and also rely on natural language text fragments for their event representation.

The trend of representational richness taking a backseat to scalable learning, then, seems to continue. So where do we go from here? How can we learn, at scale, a large number of rich, expressive representations of temporally related events and complexly related participants in those events? We introduce such a representation, and broadly discuss how to automatically knowledge with it, in Chapter~\ref{chap:schemas}. First, however, we discuss the lower level semantic representation on which it is built: Episodic Logic.