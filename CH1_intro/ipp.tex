\section{The Integrated Partial Parser (IPP)}
\label{sec:ipp}

One of the first computational applications of the theory of scripts in \citep{schankandabelson}---as well as the theory of memory Schank presented in \citep{schank1982dynamic}---is the \textbf{Integrated Partial Parser} (\textbf{IPP}), first presented by Michael Lebowitz, a student of Roger Schank's, in his dissertation \citep{lebowitz1980}. \footnote{A contemporary application of \citep{schank1982dynamic} is the \textbf{CYRUS} fact retrieval system developed by Janet Kolodner \citep{kolodner1983maintaining}, which I don't describe in this paper.} IPP's main focus is on the integration of \textit{episodic memory}---a term coined by Endel Tulving to describe the comprehensive memory of an event, as opposed to the explicit knowledge of declarative facts, which he termed \textit{semantic memory} \citep{tulving1972episodic}---into conceptual understanding and generalization processes. Schank argued, in \citep{schankandabelson} as well as many future works, that episodic memory must be organized around ``stereotyped'' episodes---i.e., scripts---for the purposes of both efficient lookup \textit{and} reasoning about new situations that seem to ``follow'' the scripts. IPP provides a concrete first computer implementation of this sort of general episodic memory, and demonstrates its potential for generalization.

\subsection{Outline of the Approach}

IPP organizes its episodic memory in terms of two fundamental types of unit. The first is called the \textbf{Simple Memory Organization Packet} (\textbf{S-MOP}), and is inspired by the Memory Organization Packets defined in \citep{schank1980language}. S-MOPs correspond to the general notion of schemas: they are packets of information representing generalized, stereotypical events (i.e., scripts). The other type of unit is the \textbf{Action Unit} (\textbf{AU}), which represents a concrete event, though not necessarily an instance of an S-MOP. AUs resemble frames in the Minsky sense: they are attribute-value structures.

\textit{N.B.: in all of the literature on IPP, the term ``Action Unit'' is used to describe both the specification of the AU's attributes} and \textit{any concrete instantiation of an AU specification. For the remainder of this section, I will use the terms} ``AU template'' \textit{and} ``AU instance'' \textit{.}

An example of two AU instances is shown in Figure~\ref{fig:ippaus}.

S-MOPs comprise three core sections, each of which is hierarchical in that it may be composed of other S-MOPs, or of AUs.
%The AUs are ``atomic'', and thus somewhat of an analog to the active conceptualizations in Schank's theory, although they may represent much more complex actions or situations.
The three core sections of an S-MOP are:

\begin{enumerate}
    \item \textbf{Methods}: this section represents ``the way the S-MOP is carried out'', and is analogous to the \textit{track} in Schankian scripts. For example, the S-MOP \texttt{S-EXTORT} might have the AU \texttt{\$HIJACK} as its method. An S-MOP only has one method.
    
    \item \textbf{Results}: this section represents the effects made true when the event described by an S-MOP is complete. Example results for \texttt{S-EXTORT} are the AUs \texttt{GS-RELEASE-HOSTAGES} and \texttt{GS-GET-RANSOM}.
    
    \item \textbf{Scenes}: this section represents events that ``often occur'' in instances of the S-MOP; it is analogous to the scenes in Schankian scripts. Scenes are an optional component of an S-MOP: they are not required to understand the nature and effects of the S-MOP, but can provide additional information on it. Example scenes for \texttt{S-EXTORT} are the AUs \texttt{G\$-NEGOTIATE} and \texttt{G\$-SIEGE}.
\end{enumerate}


\begin{figure}
    \centering
  \begin{SaveVerbatim}{AUs}
$HIJACK
    ACTOR       = *gunman*
    PLANE       = *Portuguese 727*
    PASSENGERS  = *83 passengers*

GS-GET-RANSOM
    ACTOR       = *gunman*
    AMOUNT      = *$10 million*
\end{SaveVerbatim}
  \setlength{\fboxsep}{5mm}
  \scalebox{0.75}{\fbox{\BUseVerbatim{AUs}}}
    \caption{Two example instances of IPP's action units (AUs).}
    \label{fig:ippaus}
\end{figure}

\begin{figure}
    \centering
  \begin{SaveVerbatim}{AUs}
ACTOR       [terrorist]
PASSENGERS  [group of people]
VEHICLE     [airplane]
TO          [city or country] [prep "to"]
FROM        [city or country] [prep "from"]
\end{SaveVerbatim}
  \setlength{\fboxsep}{5mm}
  \scalebox{0.75}{\fbox{\BUseVerbatim{AUs}}}
    \caption{The rules for filling the roles in the \texttt{\$HIJACK} AU}
    \label{fig:ippaurolefill}
\end{figure}

\subsection{Story Parsing}
To understand stories, IPP must convert them into its memory representation. They analyze the story with a syntactic parser, which identifies AUs by keywords, and then makes use of more keywords, as well as syntactic features, to fill their roles. An example of a rule specification for filling the roles of the \texttt{\$HIJACK} AU is shown in Figure~\ref{fig:ippaurolefill}. When processing the story \texttt{A JORDANIAN GUNMAN HIJACKED A KUWAITI JETLINER WITH
112 PASSENGERS ABOARD THURSDAY AND FORCED IT TO FLY TO
BAHRAIN}, the word \texttt{\textbf{HIJACKED}} triggers an instantiation of the \texttt{\$HIJACK} AU. The subject of the sentence is assigned to the \texttt{ACTOR} slot. ``Jetliner'' is recognized as an airplane, and is assigned to the ``VEHICLE'' slot. The means by which ``Jetliner'' is known to be an airplane are unclear, but alluded to in various sections of the dissertation: words can store pointers to other words as synonyms, and things like common names can be recognized to satisfy a ``person predicate''; in any case, this lexical knowledge is hardcoded into the system.

S-MOPs may be instantiated by lexical ``triggers'' in similar ways, and their instantiation triggers the attempted instantiation of their components, which are either more S-MOPs or AUs. There are several complex heuristics for attaching the AUs to S-MOPs. I won't detail the specifics of these rules here; for our purposes, it suffices to note that these rules exploit the hierarchical nature of composed S-MOPs, and the shared role fillers between them and their children, to identify the children in a ``top-down'' manner, or to attach previous ``orphan'' instances to their relevant S-MOPs.

\subsection{Generalization}
Instantiated S-MOPs are generalized into something called \textbf{spec-MOPs} (specialized S-MOPs), which represent the general class shared by several concrete instances of an S-MOP. For example, one of their results was the \texttt{ITALY-KID-GEN} spec-MOP, derived from several \texttt{S-EXTORT} instances of stories about kidnappings in Italy. The spec-MOP generalization algorithm works on the basis of the storage system used to index S-MOPs, and their instances, by similarity.

S-MOP instances are stored in a structure called a \textit{discrimination net} \citep{Charniak:1980:AIP:576572}, a branching graph of predicates that classifies items by successively applying the predicates to move them along the path to their class. It can be thought of as a generalization, to directed acyclic graphs and $n$-ary branch orders, of a decision tree. S-MOP instances are indexed by adding them to this structure as terminal leaves, with the predicate branches given by tests for their features. The logarithmic depth of a balanced discrimination net enables efficient retrieval of S-MOP instances, and thus of the S-MOPs they instantiate: few features need to be tested for retrieval.

To keep the number of leaves small and ensure efficient balancing, leaf S-MOP instances are generalized based on a similarity heuristic. The heuristic used in the original system was ``share at least 4 features'' when generalizing S-MOP instances into a spec-MOP, or ``share at least 2 features'' when generalizing spec-MOP instances into another, even more specific spec-MOP. However, the author suggests that better heuristics are admitted and encouraged. The new spec-MOP template is created by removing the shared features, constraining them to their shared value, as can be seen in the generalization of \texttt{S-EXTORT} instances to \texttt{ITALY-KID-GEN} based on, for example, the shared location of Italy.

\subsection{Implications for the Natural Emergence of Schemas}
%It is extremely cool\footnote{I had originally written ``particularly interesting'' here, but I just can't write something that restrained to describe the reaction I had when I saw this connection.} that IPP's generalization procedure can be seen as a ``by-product'' of a well-known procedure for building balanced decision trees.
It is notable that IPP's generalization procedure can be seen as a ``by-product'' of a well-known procedure for building balanced decision trees.
This illustrates how the capacity for story generalization---and, later, even analogical reasoning---can \textit{emerge} from the somewhat more mundane, logistical-sounding task of optimizing the recall of specific episodes from a database. This property of IPP provides compelling computational evidence for this statement from \citep{schankandabelson}: \textit{``As an economy measure in the storage of episodes, when enough of them are alike they are remembered in terms of a standard generalized episode which we will call a script.''}.

IPP's storage-derived generalization procedure is not the only support for that statement: in a study of recollections made by former White House Counsel John Dean, Ulric Neisser compared Dean's statements with factual recordings of the events Dean claimed to be describing, and found that Dean's recollections shared many properties of the actual events, but were not specifically identical to any of them \citep{repisodic}. Neisser called this ``repisodic'' memory, and offhandedly referred to the case study as possible evidence for ``theoretical ideas about `story grammars' and `schemata'{}''. Construing Neisser's study of John Dean as evidence for schemas-as-memory in the human brain also supports an interpretation of IPP's generalization procedure as evidence for the natural emergence of schemas from efficient episodic retrieval techniques. Evidence has been presented suggesting that potential neural circuitry for episodic memory evolved slowly, in pieces, across a long timeline of ancestors and species \citep{allen2013evolution}.
%This suggests that each ``step along the way'' to our own episodic memory was locally beneficial to the survival of a species, and thus that schema-like representations in the human brain could be the result of compounded ``micro-improvements'' to our memory systems---much like IPP's generalized S-MOPs are a consequence of its efficient episode indexing system. Of course, I'm not an evolutionary psychologist, so I'll put this line of speculation to rest here.

\subsection{Additional Discussion}
%My excessive fawning \textit{supra} over possible implications of IPP's generalization scheme is not to say that it is necessarily a scalable system, as-is, for the practical task of schema learning; in fact,
The IPP model has several practical shortcomings, mostly related to the amount of hand-engineered information needed to ``bootstrap'' it. Note that IPP can only generate \textit{spec-MOPs}: \textit{specifications} of known S-MOPs. This means that the initial set of S-MOPs must, in principle, conceptually ``cover'' all schemas one wishes to generate. This scalability concern also holds for the Action Unit definitions---including the rules used to match tokens in a syntactic parse to AU roles. Converting a syntactic parse tree into a cognitive action representation requires a magnitude of semantic reasoning and knowledge about objects, and the ontologies in which those objects are embedded, that is intractable to embed by hand in the IPP's format.

Many of these bootstrapping issues have enjoyed the attention of reasonably successful contemporary research: \textbf{(1)} ``fuzzy'' semantic word representations have seen rapid improvement as distributional word embeddings, e.g. \citep{mikolov2013}, as well as in more comprehensive \textit{language models} (distributions of possible sentences), e.g. ELMo \citep{elmo}; \textbf{(2)} the problem of textual semantic role labeling given a corpus of known frames needs no longer rely on syntactic heuristics alone, as it does in IPP (see Section~\ref{sec:framenet} for examples of automatic SRL systems); and \textbf{(3)} ``world knowledge'' about objects, of the kinds that IPP requires for role filling, is beginning to accumulate, in both hyponymy ontologies (for determining ``is-a'' relationships), e.g. WordNet's hypernym graph \citep{miller1995wordnet}, and in corpora of what \citet{pustejovsky1991generative} calls ``telic'' knowledge (for determining ``what something is \textit{good for}'') \citep{yamada2004automatic}.

But IPP assumes one thing a mass schema generator cannot assume: the existence of a large number of initial schemas. We'll now look at later approaches to schema generation, and examine whether they are better suited to unsupervised schema learning.