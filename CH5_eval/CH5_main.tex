\chapter{Evaluation}

%EL schemas enable consistent, structured, and interpretable inferences about novel text, by matching pieces of the text to pieces of the schema, replacing schema variables with entities from the story, and treating other formulas in the schema that use those newly-filled variables as inferences. Here, we describe an assessment of the generality and relevance of the schema formulas learned by NESL.
%In addition to inferences, we would like to evaluate whether the schemas we obtain are both \textit{topically cohesive}, i.e., focused descriptions of one kind of situation; and \textit{interesting}, i.e., capable of generating useful and novel inferences about situations, rather than obvious or redundant ones.
This chapter describes experiments performed to assess the quality of schemas learned by NESL, and of inferences generated by learned schemas on unseen stories.
One way of evaluating acquired script-like knowledge is the \textit{narrative cloze} task, introduced by \citet{chambers2008unsupervised}, in which one event in a sequence is masked, and acquired knowledge is used to predict its identity given the rest of the sequence.
The narrative cloze task has been widely adopted as a standard evaluation for script-like knowledge acquisition \citep{jans-etal-2012-skip,rudinger-etal-2015-learning,pichotta2016learning,lee-goldwasser-2019-multi}.
However, \citet{rudinger-etal-2015-learning} note that the format of the narrative cloze task essentially mirrors that of a language model objective function, in which a missing token is predicted given a preceding sequence (in the autoregressive objective) or surrounding sequences (in the masked objective).
They argue that either discriminative language models are sufficient for the acquisition of script knowledge, or the narrative cloze task is insufficient for its evaluation.
\citet{chambers-2017-behind} defends the event cloze task by pointing out that many of its implementations, in automatically generating their evaluation sets, tested for non-representative distributions of events favoring high-frequency ones.
They argue for manual construction of narrative cloze tests by human annotators, with an emphasis on the inclusion of \textit{core} script events.

While NESL uses a generative language model to produce stories as an initial source for schema knowledge, the knowledge encoded in the final schemas is not drawn from the same distribution as the one producing the stories; stories often elide basic, but important, information about goals, preconditions, postconditions, and general semantic role types, which we introduce via protoschema identification. Furthermore, generative models of stories may call for the introduction of surprising events or facts in accordance with larger-scale \textit{narrative} schemas---this to the semantic detriment of the more basic, unsurprising schemas we'd like to extract from stories. Because of these factors contributing to the potentially highly disjoint event spaces of individual stories and learned schemas, we believe that a narrative cloze task is insufficient to test the quality and utility of the inferences generated by NESL-learned schemas, and employ human evaluation of learned schemas and their inferences. The remainder of this chapter describes the evaluation of schema steps and goals given the schema topic (Section~\ref{sec:schema_eval}), and the evaluation of concrete inferences, drawn using learned schemas, about unseen stories (Section~\ref{sec:inf_eval}).

\section{Schema Evaluation}
\label{sec:schema_eval}
Our first study evaluated the quality of individual schema formulas learned by NESL. We presented learned schema formulas from the \texttt{:Steps} and \texttt{:Goals} sections to human annotators and asked them to rate the quality of the formula, given the schema topic, on a 1-5 Likert scale. The prompt given to the evaluators was the following:
\begin{displayquote}
\textit{The statement above is a reasonably clear, entirely plausible general claim and seems neither too specific nor too general or vague to be useful.}
\end{displayquote}
This prompt is due to \citet{knext-eval}, who used it to elicit evaluations of quality of automatically acquired knowledge formulas from the \texttt{KNEXT} acquisition system. It incorporates three key dimensions into one quality rating: \textbf{clarity} of the representation, and \textbf{plausibility} and \textbf{generality} of the claim.

Prior work on human evaluation of automatically acquired script-like knowledge has used comparable study design. We compare our results to \citet{pichotta2016learning}, whose inferred event tuples were rated on a Likert scale, with values ranging from \textit{Very Unlikely} to \textit{Very Likely}, by human evaluators. While the semantics of the prompt are more ambiguous than our \texttt{KNEXT}-derived prompt, the normalized results are more or less comparable. \citet{weber_causal_scripts} evaluate acquired knowledge using two metrics, one of which is the \textit{event chain completion} metric, in which human evaluators rate the likelihood of a predicted event given a chain of preceding events. The comparison of this metric to ours is arguably the least direct, as we evaluate the likelihood of an event predicated on the truth of the schema's \textit{topic}; however, under the assumption that the given events in the event chain completion metric sufficiently characterize an implicit schema topic to the human evaluator, a comparison is possible. \citet{goal-oriented-scripts} ask human evaluators to \textit{edit} learned scripts by either re-ordering their steps or deleting irrelevant steps. This leads to an approximation of schema relevance defined by the ratio of edited script length to original script length, i.e. how many of the learned events were judged to be irrelevant. \citet{starsem-scripts} manually evaluate their own script events for topical relevance, among other things.

For each step and goal formula in each learned schema, we first constructed a \textit{tabular representation}, meant to render the EL formula in a human-readable way while still illustrating its formal predicate-argument structure.

\begin{figure}
    \centering
    \includegraphics[width=0.5\textwidth]{CH4_learning/evaleg3.pdf}
    \caption{An example of a form presented to a schema quality evaluator for a \textit{step} formula.}
    \label{fig:step_eval_eg}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=0.5\textwidth]{CH5_eval/gpteval}
    \caption{An example of a form presented to a schema quality evaluator for a \textit{step} formula that has been rendered, using GPT-2, into natural English. This corresponds to the \textit{tabular} version shown in Figure~\ref{fig:step_eval_eg}.}
    \label{fig:gpt_step_eval_eg}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=0.5\textwidth]{CH4_learning/evaleg4.pdf}
    \caption{An example of a form presented to a schema quality evaluator for a \textit{goal} formula.}
    \label{fig:goal_eval_eg}
\end{figure}

\section{Inference Evaluation}
\label{sec:inf_eval}

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{CH5_eval/role_inf.pdf}
    \caption{An example of a form presented to a schema quality evaluator for a \textit{goal} formula.}
    \label{fig:role_inf_eval}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=0.75\textwidth]{CH5_eval/step_inf.pdf}
    \caption{An example of a form presented to a schema quality evaluator for a \textit{goal} formula.}
    \label{fig:step_inf_eval}
\end{figure}