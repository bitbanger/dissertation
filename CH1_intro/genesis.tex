\section{GENESIS}
\label{sec:genesis}

Most approaches to schema acquisition have used statistical extraction models to cope with the vast amount of natural language text needed for automatic generalization. However, the GENESIS system, first presented in Ray Mooney's dissertation \citep{mooney:phd88}, attacks the schema learning problem symbolically, generalizing causal, structured schemas  without the need for a huge number of examples.
%Automatic schema acquisition work was nearly non-existent before the meteoric rise of machine learning methods inspired by developments in backpropagation \citep{rumelhart1986}. In fact, the GENESIS system \citep{mooneydejong1985,mooney90} seems to be the only non-statistical attack on the problem before the ``ML spring'' following the AI winter.

GENESIS generalizes schemas \footnote{I am hesitant, unlike these authors, to use the Greek pluralization rule for the word \textit{schema}.} from single narratives using \textit{explanation-based learning} \citep{mitchell1986}, a symbolic learning method which uses a set of symbolic rules to ``explain'' observed actions, thereby offering clues for what is generalizable without compromising the underlying teleology of the narrative. \footnote{As \citep{mooney90} notes, explanation-based learning can be viewed in a dichotomy with \textit{similarity-based learning}, which depends on a large number of examples and a similarity metric to learn generalized structures---neural networks and other statistical schema learning approaches, to be detailed in further sections of this chapter, are examples of this latter end of the dichotomy.}

\subsection{Outline of the Approach}
\label{sec:genesisapproach}

At its core, GENESIS is a cyclic combination of classical planning and pre-existing schema matching techniques. A schema can only be used to interpret a teleological sequence of actions (i.e. a \textit{plan}) if that schema is already known. Absent any relevant schemas, GENESIS resorts to classical situation planning techniques, manipulating an internal logical representation of a story to generate a plan, composed of known operations, that aligns with the character's actions in the story. If such a story-explaining plan is found, it is then analyzed, and novel action sequences necessary to achieve important goals are generalized and stored as schemas.

The technique of storing plans to enable ``sub-sequence'' analysis is not new: the STRIPS problem solver system stored generalized plans in just such a way using \textit{triangle tables}: data structures that allowed efficient determination of the next applicable step of a multi-step plan given an arbitrary world state \citep{fikes1972}. In fact, this similarity is the result of direct inspiration: GENESIS's internal logical representation and planning algorithm are modified versions of those used in STRIPS.

\subsection{Knowledge Representation}

GENESIS first converts stories into an internal knowledge representation, whose ontology comprises \textit{object}, \textit{attribute}, \textit{state}, and \textit{action} classes. \textit{Objects} are atomic, symbolic representations of entities in the planning domain, e.g. \textit{sunrise} or \textit{Jacob}. Note that these are not necessarily \textit{physical} entities, but rather, the entire ``domain'' of the logical system in which the planning inferences occur. \textit{Attributes} are ``rules'' about objects that are unaffected by actions. \textit{States} are conjunctions of temporal propositions representing the world at a given time. \textit{Actions} are temporal predicates endowed with axioms whereby associated \textit{preconditions} are implied by the action predicate's truth at some time, associated \textit{postconditions} are implied at some time after the action, and associated \textit{motivations} for the action are implied of relevant entities by the action's truth at some time.

The ``frame problem'' \citep{mccarthy69} of axiomatizing the temporal persistence of these predicates is handled by the ``STRIPS assumption'' \citep{fikes1972}: stated simply, propositions may be added to or deleted from the working knowledge base at some timestep \footnote{Under the \textit{closed-world assumption} \citep{Reiter:1987:CWD:42641.42663}, we assume that any predicates not present in the knowledge base are false; the knowledge base is parameterized by a discrete time variable.} if and only if an action occurred with those postconditions at that timestep.
%$Believe(?x, Occupation(?y, professor)) :- At(?x, ?l), At(?y, ?l), Isa(?l, university), Attire(?y, ?c), Official(?c)$.

\begin{figure}
\begin{tabular}{ |p{3cm}|p{2cm}|p{3.5cm}|p{4cm}|  }
 \hline
 \multicolumn{4}{|c|}{\textbf{Arrest(?a,?b,?d(?b))}} \\
 \hline
 \textbf{Constraints}&\textbf{Preconds}&\textbf{Motivations}&\textbf{Postconds}\\
 \hline
Isa(?a,Character)&At(?b,?l)&Believe(?a,?d(?b))&Arrested(?b,?d(?b),?a)\\
Isa(?b,Character)&At(?a,?l)&Occupation(?a,cop)&\\
$\lnot$Equal(?a,?b)&&&\\
Illegal(?d(?b))&&&\\
 \hline
\end{tabular}
\caption{A table from \citep{mooney90} illustrating an \textit{Arrest} example action in the GENESIS representation. Constraints are atemporal and checked against state-independent attributes, whereas preconditions are temporal and state-dependent.}
\label{fig:genesisaction}
\end{figure}

\subsection{The Planning Process}

Because GENESIS is a schema learning system, it deals primarily with \textit{action}-type knowledge; an example action, taken from \citep{mooney90}, is shown in Figure~\ref{fig:genesisaction}. If GENESIS encounters an action in a story that does not match an existing (or instantiable) schema, it will attempt to deduce a teleology, via planning, to explain the action with past actions. The unknown action's preconditions are taken as goals, and a backward-chaining inference process begins on the Horn clauses in the knowledge base at the action's time: the system must prove that the preconditions and motivations of the unknown action were made true by the postconditions of past actions.

The authors note that motivations are seldom made explicit, and so they manually encode prior \textit{thematic goals} \footnote{(defined by \citep{schankandabelson} as atomic goals, derived from atomic needs and wants, which require no explanation)} into GENESIS. An example of a thematic goal, taken from their paper, is a police officer's desire to arrest lawbreakers: \texttt{Arrested(?b,?d,?a) $\land$ Occupation(?a, cop) $\implies$ ThemeGoalMet(?a, Arrested(?b, ?d, ?a))}.

\subsection{Schema Generalization}

Once the process of achieving a goal has been explained, GENESIS uses three criteria to determine whether to generalize the explanation into a schema, in order to keep the massive number of possible generalizations to a useful minimum. These criteria are designed to encourage the generation of novel ``end-to-end'' schemas that one character can undertake as composite actions in order to achieve an inherently useful and interesting goal. They are:

\begin{enumerate}
    \item The goal achieved should be a thematic goal, rather than a composite goal.
    \item The explanation should not just be an instantiation of an already learned schema.
    \item The top-level actions of the explanation should be undertaken by the character whose goal we've explained.
\end{enumerate}

If an explanation meets these criteria, GENESIS ``prunes'' the explanation, removing irrelevant and/or overly restrictive facts. For example, a top-level explanation of an arrest might have four steps: a police officer goes to a known drug producer's house, the police officer tricks the drug producer into letting them inside, the police officer observes the drug lab, and, finally, the police officer makes an arrest, satisfying a thematic goal. However, these actions are composite: perhaps the police officer tricks the drug producer into letting them inside by pretending to be their uncle. Pretending to have a false identity may be essential to satisfying a constraint that A only lets B in their house if A thinks B is trusted. However, the fact that uncles are trusted is not necessary in this causal graph, and may thus be pruned for generalization.

Finally, GENESIS generalizes the schema by removing unnecessary restrictions using a generalization algorithm (EGGS) given by \citep{mooney:phd88} \footnote{(which is itself derived from the STRIPS generalization algorithm given by \citep{fikes1972})}. Mooney gives the following example plans to help illustrate what restrictions are ``necessary':

\begin{itemize}
    \item \texttt{GoThru(Door1,Room1,Room2)}
    \item \texttt{PushThru(Box1,Door1,Room2,Room1)}
\end{itemize}

With the constant-to-variable substitutions \texttt{Door1 $\gets$ ?d}, \texttt{Room1 $\gets$ ?r1}, \texttt{Room2 $\gets$ ?r2}, and \texttt{Door1 $\gets$ ?b}, the resulting plan is \textit{too specific}: the preconditions of the operators, as defined earlier in that paper, do not require that the robot push the box into the same room it started in. However, consider this plan:

\begin{itemize}
    \item \texttt{GoThru(Door1,Room1,Room2)}
    \item \texttt{PushFromRoom2ToRoom1(Box1)}
\end{itemize}

The conditions for \texttt{PushFromRoom2ToRoom1} do, in fact, require that the robot start in room 2 and end in room 1. So, simply replacing the constants with variables is \textit{too general}: the rooms \textit{must} be Room1 and Room2. So, the algorithm must replace constants by variables whenever possible, subject to the constraint that the pre- and post-conditions of the relevant operators are respected.

It turns out that the algorithm for doing this involves unifying all possible pairs well-formed formulas (WFFs) in the logical representation using the unification algorithm given by \citep{robinson1965machine}. The resulting unifications (which can be thought of as bindings, or substitution maps) are then applied to the entire explanation structure. Once this is done, GENESIS has produced a generalized schema for doing ``the kind of thing'' a character did in the story to achieve their goal.

\subsection{Example Generalization}
\label{sec:genesiseg}

The authors present a ``case study'' of GENESIS wherein a ``learning narrative'', which described a schema instance in great detail, was presented to the system, followed by a ``test narrative'', which described another instance of the same schema but left out many important actions. The system was able to apply the generalized schema it obtained from the learning narrative to the test narrative, constructing an explanation from very sparse information.

To show that the test narrative was insufficient for question-answering purposes, the authors published the following interaction with GENESIS on the basis of their input (the test narrative). This is shown in Figure~\ref{fig:genesisqa1} of Appendix~\ref{app:genesisqa}. After presenting it with the learning narrative (Figure~\ref{fig:genesisqa2}), it generalized a schema, and applied that schema to the test narrative to fill in the story details (Figure~\ref{fig:genesisqa3}).

\subsection{Discussion}

The generalized schema shown in figure~\ref{fig:genesisqa2} seems quite comprehensive: in addition to ``type'' predicates on its entities, e.g. \texttt{?b9 is a person}, it also relates those entities via actions, e.g. \texttt{?s9 goes from ?s4 to ?l4}. Furthermore, the actions are not simply subject-verb-object triples: they specify pre- and post-conditions, as well as motivations, all of which contribute to the complexity and goal-orientedness of the final schema. However, I do want to note some important concerns related to the scalability of its schema learning.

The stories that GENESIS learned from were artificial, containing very little extraneous detail and really instantiating only one substantial schema. Real stories often have many simultaneously active schemas, as well as details and stylistic artifacts that complicate schema recognition. GENESIS's causal explanation-based generalization does seemingly deal with some of these issues, but its efficacy on real, ``dirty'' stories has not been demonstrated at scale.

The causal explanations that GENESIS relies on for schema construction also rely on a significant corpus of hand-generated actions, which specify preconditions, postconditions, and goals. The STRIPS-style, first-order predicate logic these actions are written in also lacks the expressive power of natural language. This all limits GENESIS rather severely: without a complete understanding of the world-meaning of every action verb it encounters, GENESIS is unable to generalize schemas from natural language. This sort of complete understanding would be very difficult to give a schema learning system \textit{a priori}.

GENESIS (section~\ref{sec:genesis}) was a rather compelling attack on the task of schema learning, but I do think that these issues ultimately lead to poor scalability.