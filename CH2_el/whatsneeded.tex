\section{What's needed for schema learning?}
\label{sec:headstart}
Two components are necessary for schema learning by definition: a model of schemas to learn, and an algorithm to learn them. These cannot be selected independently: as we saw in IPP (Section~\ref{sec:ipp}), the knowledge representation, and the knowledge stored in it, informs even the lowest levels of the learning algorithm, i.e. the syntactic parsing process. A third component that is almost certainly necessary for practical learning is \textit{a priori} common-sense world knowledge: unless one can implement Turing's ``child machine'' \citep{turing1950}---a robot that is a complete ambulatory and sensory facsimile of a human child---schemas must be learned from more indirect sources, like natural language texts, and so some corpus of knowledge is necessary to ``bridge'' purely lexical information with the sorts of observations children would get by virtue of their eyes and ears (e.g. vacuum cleaners are loud, loudness is unpleasant). In other words, we must ``level the playing field'' to give our schema learning systems similar capabilities as the most powerful learning systems we know of: human children.

As my advisor says frequently: \textit{``Babies do not start intellectually as tabulae rasae}\textit{.''} Here, I'll introduce a few capabilities that our group considers to be a necessary ``head start'' for a scalable schema learning system---and some evidence for the presence of these capabilities in actual humans.

\subsection{Symbolic representations}
Although the debate between \textit{connectionist} (i.e. neural, emergent) models of human cognition vs. \textit{classical} (i.e. symbolic, logical) ones rages on, there is evidence that symbolic representations, and symbolic reasoning, exist in the human brain---regardless of the nature of its ``implementation''. In an fMRI study conducted at Purdue \citep{siskind2015}, Jeffrey Mark Siskind's group created a simple space of sentences with 4 possible actors, 3 possible verbs (\textit{carry, fold, leave}), 3 possible objects (\textit{chair, shirt, tortilla}), and 2 directions (\textit{on the left, on the right}), for a total of 72 possible sentences. They recorded videos representing each of these sentences, and took fMRI brain image sequences of human subjects watching these videos.

Using SVM classifiers, they attempted to decode brain image sequences to determine each of the video components independently (actor, verb, object, and direction), as well as the pairs, triples, and full sentences. They were able to determine all combinations of components with accuracy well above chance. Additionally, they trained fMRI classifiers to determine the same set of actions from brain activity when viewing text input \textit{and}, separately, when viewing video input. They were able to get comparable, above-chance accuracy when decoding verbs from images of one modality \textit{using the classifier trained on the other modality}.

Their findings strongly suggest that the brain not only uses compositional action representations (i.e., with ``slots''), but that the areas in the brain that store these representations are common to both text \textit{and} video input---in other words, the brain develops a common symbolic representation of its observations. These results strongly suggest that the brain uses symbols to store information about the world, and these symbols can be generated from many kinds of observation.

\subsection{Slot relationships}
The ``slot-and-filler'' structure of Minsky's frames (see Section~\ref{sec:minsky}) has informed much work on frame-like ontologies. Modern implementations of these frame-like ontologies, such as the description logic \textbf{OWL (Web Ontology Language)} \citep{mcguinness2004owl}, have an analyzable, formal attribute-value semantics, and support slot type constraints, inheritance, composition, and many other useful forms of taxonomic reasoning. However, description logics like \textbf{OWL} do not support the expression of formal, complex \textit{relationships} between their slots: in a frame for a children's school, for example, with slots for a principal and a vice principal, modern frames generally could not formally impose inter-slot constraints such as ``the principal is the boss of the vice principal''. FrameNet (Section~\ref{sec:framenet}) does relate its slots to one another, but only informally, with natural language. Any schema representation we choose to \textit{learn} mechanically should support these sorts of slot relationships with a semantics that can be \textit{reasoned about} mechanically.

\subsection{Branching, conditionals, and variables}
When someone is working to cook a dish, they don't only follow a series of steps. They may have to \textit{branch} (e.g. \textit{if the dough looks wet, lay it on the table and wait five minutes}), execute \textit{loops} (e.g. \textit{check the dough every five minutes, removing it when it appears golden brown}), and manipulate named variables (e.g. \textit{note the weight of the dough in grams; when it's done baking, allow it to cool for half as many minutes as its weight}). In this sense, the schemas that describe our behavior are quite comparable to computer programs, and their representation should facilitate these operations.

\subsection{Time, probability, and meta-reasoning}
Schemas control human behavior in the real world, and so some important aspects of that world should be easily expressible in the language as well. The real world is inherently \textit{temporal}, and so we should be able to express relative times (e.g. \textit{it happened before that}) as well as specified magnitudes (e.g. \textit{it happened two weeks ago}). We also need to express \textit{probabilities and certainties} (e.g. \textit{he has a 50\% chance of winning}, or \textit{it is possible that it will rain tomorrow}). Finally, humans operate in a world with other self-motivated agents, including other humans, and as such must reason about the knowledge and beliefs of others, as well as knowledge transfer mechanisms such as speech. So, whatever form we choose for our schemas should facilitate \textit{attitudinal} inferences such as \textit{John believes that the dough is done}. As we'll see later, certain logical forms have less trouble than others expressing statements of these kinds.
    
\subsection{Analog representations}
Although Minsky discusses applying symbolic frames to 3D scene understanding, human beings seem to have especially strong, unconscious intuitions about certain physical and spatial properties. 3D object representations are suspected to be analog in humans, that is, in some sort of spatial correspondence with the objects themselves: the time humans take to mentally rotate 3D images is proportional to the degree of rotation \citep{cooper1973chronometric}. Additionally, object representation is suspected to occur largely in a dedicated brain area \citep{rosenberg2013visual}. Spatial reasoning is an extremely important aspect of story understanding, as stories take place in a physical world. And, owing to the computational difficulty of spatial reasoning, we should try to separate these ``specialist'' systems from the symbolic machinery, and optimize them independently, to whatever extent we can.
    
\subsection{Behavioral axioms}
\label{sec:schema_behavior}
Even human newborns cry for food. This is notable: being hungry is unpleasant, and even in the earliest days of their lives, human beings are ``pre-programmed'' with desires to avoid certain displeasures, to seek certain rewards, and to pursue actions---even highly primitive, instinctive ones like crying---that help realize those desires. This kind of ingrained tendency toward goal-driven action is central to understanding one's own actions, as well as the actions of others.
    
\subsection{Primitive actions}
\label{sec:schema_primitives}
OK, \citep{schank1975concdep} may have something of a point. There do seem to be certain primitive actions---crying, laughing, grasping, holding, eating---that infants start out with, experiment with, and compose to achieve basic goals. However, the key distinction from the primitive actions of Schank's CD theory is that these actions form a \textit{``starter pack''}, rather than an \textit{atomic representation}: one cannot compose all actions from what babies know how to do, and, even in the cases where one can, one still does not conceptualize complex actions like ``fishing'' in terms of thousands of separate grasping actions. However, some basic actions do seem to give a good head start to infants.
    
\subsection{Language learning}
Young humans acquire language rapidly, and language can then be used to learn nearly arbitrary information. Siskind's work, described above, suggests that our linguistic representations are split into symbols that are then used to represent entities and actions across verbal and visual modalities. Language is important for learning---especially when your goal is to learn from texts, which are more plentiful, and arguably easier to extract features from, than richer modalities such as videos.

\section{Implementing what's needed}
Imbuing a computer with any of the capabilities listed above is no small task, but learning them from scratch in a neural model seems especially daunting: how could we acquire the data needed to learn these things at the scale needed to cover the vast number of possible situations that exist? Our research group, led by Lenhart Schubert at the University of Rochester, is pursuing alternative approaches to constructing this ``head start'', such that schema learning can proceed from a small number of examples (relative to the data needed to train neural networks). Georgiy Platonov is pursuing computational understanding of spatial descriptions \citep{platonov2018computational}; Gene Kim is developing a semantic parser for mapping English to an intermediate semantic representation that directly enables certain discourse inferences while also providing a starting point for transduction into a nonindexical\footnote{Details of what this means are available in \citep{kim2019type}; here, I'll just say that ``deindexing'' includes tasks like converting the speaker-relative tense information in sentences to explicit, atemporal statements about individual episodes.} logical language \citep{kim2019type}; and together with his students, Lenhart Schubert has spent years developing that formal logical language, known as \textbf{Episodic Logic (EL)} \citep{hwang1993}, hypothesizing that the semantic types available in this language are the ones innate in human ``mentalese''.

As I join my advisor and colleagues to pursue the eventual goal of scalable schema acquisition, I will---and, in fact, have already begun to---make heavy use of Episodic Logic to define a schema representation and enable powerful inferences on, and generalizations of, schemas in that representation. So, before I detail our approach to the schema learning problem, and the work I've done toward that approach so far, I will briefly describe EL.