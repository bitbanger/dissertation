\section{Inference Evaluation}
\label{sec:inf_eval}

Making situational inferences from partial information is an important downstream use case of schemas. In addition to evaluating the quality of the steps of learned schemas in an ungrounded setting, we evaluate the quality of inferences made about unseen stories using learned schemas. As shown in Figures~\ref{fig:step_inf_eval}~and~\ref{fig:role_inf_eval}, we generate and evaluate two kinds of inferences: \textbf{step inferences}, in which steps from a learned schema are inferred on the basis of having their arguments bound by another, matched step; and \textbf{role inferences}, in which the types of entities are inferred when those entities are bound to arguments in a matched step. Inferred steps are presented to evaluators as tabreps, and inferred roles are presented by filling in the slots in the template sentence ``\textit{In this} \texttt{SCHEMA\_TOPIC} \textit{scenario, the entity} \texttt{ENTITY} \textit{has the role} \texttt{TYPE}.'' For step inferences, evaluators are presented with three 1-5 Likert scales: one evaluating the \textit{clarity} of the tabrep; one evaluating the \textit{plausibility} of the inference; and one evaluating the \textit{relevance} of the inference, i.e., the degree to which making the inference demonstrates substantial knowledge of the underlying situation. For role inferences, evaluators receive the same Likert scales, with the exception of the \textit{clarity} scale, due to the constrained nature of the template sentence being evaluated.


\FloatBarrier
\subsection{Datasets}
Two datasets are involved in the inference experiment: the set of schemas learned by NESL, and the set of unseen stories for which inferences will be drawn from the corresponding learned schemas. As this experiment only tests inference, and not schema identification, each learned schema is only used to draw inferences on a story that is known to fit the schema's topic already. In order to ensure the unseen stories conceptually fit the schemas, we draw them from the same LSS distribution---modeled by GPT-J 6B---as the stories used for the initial schema learning. In other words, we sample $N+1$ stories during LSS, learn schemas from the first $N$ using NESL, and draw inferences on the final, held-out story for evaluation.

Like in the ungrounded schema evaluation experiment, a research assistant filtered sensible story topics produced by a prompt-based ROCstory summarization model; the 42 schema topics thus chosen are as follows:

\scalebox{0.75}{
\vbox{
\begin{multicols}{3}
\begin{itemize}
\footnotesize
\item \texttt{being in a boat}
\item \texttt{breaking an arm}
\item \texttt{casino}
\item \texttt{church}
\item \texttt{coming home late}
\item \texttt{dogsitting}
\item \texttt{doing homework}
\item \texttt{driving}
\item \texttt{eating at a restaurant}
\item \texttt{eating with friends}
\item \texttt{falling down}
\item \texttt{farming}
\item \texttt{feeling sick}
\item \texttt{flying in an airplane}
\item \texttt{freezing weather}
\item \texttt{getting a haircut}
\item \texttt{giving medicine}
\item \texttt{giving someone a ride}
\item \texttt{going to work}
\item \texttt{growing older}
\item \texttt{having a party}
\item \texttt{having children}
\item \texttt{having pets}
\item \texttt{ice skating}
\item \texttt{jail time}
\item \texttt{jogging}
\item \texttt{meeting new people}
\item \texttt{moving into a new house}
\item \texttt{planting a tree}
\item \texttt{playing an instrument}
\item \texttt{playing in the sand}
\item \texttt{reading a book}
\item \texttt{seeing someone you know}
\item \texttt{sleeping}
\item \texttt{starting a family}
\item \texttt{swimming}
\item \texttt{training an animal}
\item \texttt{traveling}
\item \texttt{waking up}
\item \texttt{walking a dog}
\item \texttt{watching a show}
\item \texttt{working out}
\end{itemize}
\end{multicols}
}
}

\begin{figure}
    \centering
    \includegraphics[width=0.5\textwidth]{CH5_eval/role_inf.pdf}
    \caption{An example of a form presented to a schema quality evaluator for a \textit{goal} formula.}
    \label{fig:role_inf_eval}
        \includegraphics[width=0.5\textwidth]{CH5_eval/step_inf.pdf}
    \caption{An example of a form presented to a schema quality evaluator for a \textit{goal} formula.}
    \label{fig:step_inf_eval}
\end{figure}

\begin{table}[ht]
    \centering
    \begin{tabular}{l|l|l}
       \textbf{Metric} & \textbf{Average Step Score} & \textbf{Average Role Score} \\
       \hline
       Clarity & 0.79 & --- \\
       Plausibility & 0.61 & 0.78 \\
       Relevance & 0.57 & 0.66
    \end{tabular}
    \caption{Average quality scores for inferences about unseen stories generated by learned schemas and presented as tabreps (for steps). Scores have been normalized to a 0-1 range. No clarity measurements were performed for role inferences due to their constrained display representation.}
    \label{tab:inf_results}
\end{table}